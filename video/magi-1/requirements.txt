--extra-index-url https://flashinfer.ai/whl/cu124/torch2.4/

pydantic >= 2.0.0
inferencesh

torch
accelerate==0.32.1
beautifulsoup4==4.13.4
debugpy==1.8.14
diffusers==0.29.2
einops>=0.6.0
ffmpeg-python
# flash-attn==2.4.2
https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.7.4.post1+cu126torch2.4-cp312-cp312-linux_x86_64.whl

flashinfer-python==0.2.0.post2
ftfy==6.2.0
gpustat==1.1.1
imageio==2.34.0
imageio[ffmpeg]
matplotlib==3.10.1
numpy==1.26.4
protobuf==5.28.3
rich==14.0.0
sentencepiece==0.2.0
timm==1.0.15
torchdiffeq==0.2.4
transformers==4.42.3
