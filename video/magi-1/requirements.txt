--extra-index-url https://flashinfer.ai/whl/cu124/torch2.4/

pydantic >= 2.0.0
inferencesh

torch
accelerate
beautifulsoup4==4.13.4
debugpy==1.8.14
diffusers
einops>=0.6.0
ffmpeg-python
# flash-attn==2.4.2
https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.17/flash_attn-2.8.3+cu128torch2.9-cp311-cp311-linux_x86_64.whl
flashinfer-python==0.2.0.post2
ftfy==6.2.0
gpustat==1.1.1
imageio==2.34.0
imageio[ffmpeg]
matplotlib==3.10.1
numpy==1.26.4
protobuf==5.28.3
rich==14.0.0
sentencepiece==0.2.0
timm==1.0.15
torchdiffeq==0.2.4
transformers==4.42.3
