pydantic==2.10.6
accelerate==1.11.0
inferencesh
torch==2.9.1
torchvision>=0.20.0
torchaudio==2.9.1
diffusers==0.35.2
transformers==4.57.1
tokenizers==0.22.1
torchaudio==2.9.1
torchvision==0.24.1
numpy>=1.23.5,<2
imageio
imageio-ffmpeg
safetensors
huggingface_hub
hf_transfer
ftfy
Pillow 
gguf
peft
kernels
https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.22/flash_attn-2.8.1+cu128torch2.9-cp312-cp312-linux_x86_64.whl