namespace: falai
name: omni-human-1-5
description: |-
    Omni-Human 1.5 - Bytedance's audio-driven video generation with lip-sync and emotion correlation. Example usecases are:

    * You have a static picture of a person and an audio file and want to make the person say what's in the audio file. This model will return a video with the person talking (extrapolated from the initial image and with some animations and lipsyncing).

    Please note that this model doesn't directly accept text in the `audio` parameter but will actually require an audio file with the text already transformed into audio; for that you can use any TTS (text-to-speech) app first like Chatterbox, Voicevibe, Elevenlabs, Dia, etc..
category: video
images:
    card: https://cloud.inference.sh/u/01sm9vzqrjkvqhzx5xsybwce0y/01k1zx0sb95jzqpnz0pw9whndn.png
    thumbnail: ""
    banner: ""
metadata: {}
variants:
    default:
        name: default
        order: 0
        resources:
            gpu:
                count: 0
                vram: 0
                type: none
            ram: 4000000000
        env:
            FAL_KEY: ""
        python: "3.10"
secrets:
  - key: FAL_KEY
